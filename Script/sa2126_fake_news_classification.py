# -*- coding: utf-8 -*-
"""SA2126 - Fake News Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1flv6SMy6G7xmMEV51XvxUVcZithrCLtE
"""

!pip install nltk

# @title Functions for handling missing values and text cleaning

# Import necessary libraries
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from datetime import datetime
from google.colab import files

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
# Download the 'punkt_tab' data package
nltk.download('punkt_tab') # This line downloads the necessary 'punkt_tab'

# Initialize lemmatizer and stop words
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Handle Missing Values
def handle_missing_values(df, columns):
    # Get the actual column names in the DataFrame
    actual_columns = df.columns.tolist()
    for col in columns:
        # Check if the column exists in the DataFrame before processing
        if col in actual_columns:
            df[col].fillna('Missing', inplace=True)
        else:
            print(f"Warning: Column '{col}' not found in DataFrame.")
    return df

# Text Cleaning
def clean_text(text):
    if not isinstance(text, str):
        return 'Missing'  # Handle non-string values
    # Remove punctuation and numbers
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)      # Remove numbers
    # Tokenize and remove stopwords
    tokens = word_tokenize(text.lower())
    tokens = [word for word in tokens if word not in stop_words]
    # Lemmatize tokens
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)

# Apply text cleaning to multiple columns
def clean_text_columns(df, columns):
    for col in columns:
        df[col] = df[col].apply(lambda x: clean_text(x))
    return df

# Load train and test datasets
train_df = pd.read_csv("train.tsv", sep='\t')
test_df = pd.read_csv("test.tsv", sep='\t')

train_df

train_df['subject'] = train_df['subject'].replace({'politicsNews': 'politics'})
test_df['subject'] = test_df['subject'].replace({'politicsNews': 'politics'})

unique_subjects = train_df['subject'].unique()
unique_subjects

unique_subjects = test_df['subject'].unique()
unique_subjects

print(train_df['subject'].value_counts())
test_df['subject'].value_counts()

missing_subjects = train_df['subject'].isnull().sum()
print(f"Number of missing values in 'subject' column: {missing_subjects}")

missing_values_count = train_df.isnull().sum()
missing_values_count

missing_values_count = test_df.isnull().sum()
missing_values_count

# @title Label Encoding of Subject column

from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the 'subject' column
train_df['subject_encoded'] = label_encoder.fit_transform(train_df['subject'])
test_df['subject_encoded'] = label_encoder.fit_transform(test_df['subject'])

# Print the mapping between original and encoded labels
subject_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
print("Subject Mapping:")
print(subject_mapping)

# Now 'train_df['subject_encoded']' contains the numerical labels
print(train_df[['subject', 'subject_encoded']].head(20))
test_df[['subject', 'subject_encoded']].head(20)

# @title Group data by 'subject' and 'label' and count occurrences
label_counts = train_df.groupby(['subject', 'label']).size().unstack(fill_value=0)

label_counts

# @title Group data by 'subject' and 'label' and count occurrences on test data
label_counts = test_df.groupby(['subject', 'label']).size().unstack(fill_value=0)

label_counts

# @title Handle missing values in critical fields
critical_columns = ['title', 'text', 'subject', 'date', 'label', 'subject_encoded']
train_df = handle_missing_values(train_df, critical_columns)
test_df = handle_missing_values(test_df, critical_columns)

# @title Date extraction
def split_date(df):
    if 'date' not in df.columns:
        print("Warning: 'date' column not found in DataFrame.")
        return df

    # Split the 'date' column based on spaces, ensuring 3 elements
    df[['month', 'day', 'year']] = df['date'].str.split(expand=True, n=2)  # Limit to 2 splits for 3 elements

    # Handle cases where there are fewer than 3 elements
    df['month'] = df['month'].fillna('Missing')
    df['day'] = df['day'].fillna('Missing')
    df['year'] = df['year'].fillna('Missing')

    return df

train_df = split_date(train_df)
test_df = split_date(test_df)

train_df['month'] = train_df['month'].str[:3]
test_df['month'] = test_df['month'].str[:3]

train_df['day'] = train_df['day'].str[:-1]
test_df['day'] = test_df['day'].str[:-1]

def combine_date_columns(df):
    """Combines day, month, and year columns into a single datetime column."""
    if not all(col in df.columns for col in ['day', 'month', 'year']):
        print("Warning: 'day', 'month', or 'year' columns not found in DataFrame.")
        return df

    # Convert 'day' and 'year' to numeric, handling errors gracefully
    df['day'] = pd.to_numeric(df['day'], errors='coerce')
    df['year'] = pd.to_numeric(df['year'], errors='coerce')

    # Fill NaN values with 1 before converting to int
    df['day'] = df['day'].fillna(1).astype(int)
    df['year'] = df['year'].fillna(2000).astype(int)  # Replace with a reasonable default year

    # Create a combined date string
    df['combined_date_str'] = df['month'].astype(str) + ' ' + df['day'].astype(str) + ', ' + df['year'].astype(str)

    # Parse the combined date string into datetime objects, handling various formats
    formats = ["%b %d, %Y", "%B %d, %Y"]  # Example formats, add more as needed
    for fmt in formats:
        try:
            df['combined_date'] = pd.to_datetime(df['combined_date_str'], format=fmt, errors='coerce')
            # If successful, break out of the loop
            break
        except ValueError:
            pass  # Try the next format
    else:
        print("Warning: Could not parse dates with any of the specified formats.")

    return df

# Apply the function to your dataframes
train_df = combine_date_columns(train_df)
test_df = combine_date_columns(test_df)

year_counts = train_df['year'].value_counts()
year_counts

# Drop rows where the 'year' column is 2000
train_df = train_df[train_df['year'] != 2000]
test_df = test_df[test_df['year'] != 2000]

# Drop 'combined_date_str' and 'date' columns
train_df = train_df.drop(['combined_date_str', 'date'], axis=1)
test_df = test_df.drop(['combined_date_str', 'date'], axis=1)

# Rename 'combined_date' to 'date'
train_df = train_df.rename(columns={'combined_date': 'date'})
test_df = test_df.rename(columns={'combined_date': 'date'})

# Create 'month_label' column based on month abbreviations
month_mapping = {
    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
    'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12
}
train_df['month'] = train_df['month'].map(month_mapping)
test_df['month'] = test_df['month'].map(month_mapping)

# Ensure 'date' column is of datetime type
train_df['date'] = pd.to_datetime(train_df['date'])
test_df['date'] = pd.to_datetime(test_df['date'])

# Extract the day of the week and create 'day_of_week' column # This line is added
train_df['day_of_week'] = train_df['date'].dt.dayofweek
test_df['day_of_week'] = test_df['date'].dt.dayofweek

# Extract the day of the week and create 'is_weekend' flag
train_df['is_weekend'] = (train_df['day_of_week'] >= 5).astype(int)
test_df['is_weekend'] = (test_df['day_of_week'] >= 5).astype(int)

# Drop specified columns
train_df = train_df.drop(['day'], axis=1, errors='ignore')
test_df = test_df.drop(['day'], axis=1, errors='ignore')

# Drop specified columns
train_df = train_df.drop(['day_of_week'], axis=1, errors='ignore')
test_df = test_df.drop(['day_of_week'], axis=1, errors='ignore')

# @title Subject_encoded vs Label
label_counts = train_df.groupby(['subject_encoded', 'label']).size().unstack(fill_value=0)
label_counts2 = test_df.groupby(['subject_encoded', 'label']).size().unstack(fill_value=0)

print(label_counts)
label_counts2

"""Too much biased distribution in both test and train data based on subject."""

# @title Year vs Label
label_counts = train_df.groupby(['year', 'label']).size().unstack(fill_value=0)

label_counts

# @title Year vs Label for test data
label_counts = test_df.groupby(['year', 'label']).size().unstack(fill_value=0)

label_counts

# @title Drop 2015 year rows as they are completely biased towards fake news

# Drop rows where the year is 2015
train_df = train_df[train_df['year'] != 2015]
test_df = test_df[test_df['year'] != 2015]

# @title Clean textual data
text_columns = ['title', 'text']  # Include 'Subject' for cleaning
train_df = clean_text_columns(train_df, text_columns)
test_df = clean_text_columns(test_df, text_columns)

# Save preprocessed data to new files
train_df.to_csv("preprocessed_train.tsv", sep='\t', index=False)
test_df.to_csv("preprocessed_test.tsv", sep='\t', index=False)

print("Preprocessed Train Data saved as 'preprocessed_train.tsv'")
print("Preprocessed Test Data saved as 'preprocessed_test.tsv'")

# @title Weekend vs Label
weekend_label_distribution = train_df.groupby(['is_weekend', 'label']).size().unstack(fill_value=0)
weekend_label_distribution

# @title Distribution of Real and Fake News by Weekend

import matplotlib.pyplot as plt

# Create a stacked bar chart
plt.bar([0, 1], weekend_label_distribution[0], label='Fake News')
plt.bar([0, 1], weekend_label_distribution[1], bottom=weekend_label_distribution[0], label='Real News')

# Customize the chart
plt.xticks([0, 1], ['Weekday', 'Weekend'])
plt.xlabel('Day Type')
plt.ylabel('Number of Articles')
plt.title('Distribution of Real and Fake News by Weekend')
_ = plt.legend()

# @title Weekend vs Label for test data
weekend_label_distribution = test_df.groupby(['is_weekend', 'label']).size().unstack(fill_value=0)
weekend_label_distribution

# @title Distribution on weekends and week days for test data
plt.bar([0, 1], weekend_label_distribution[0], label='Fake News')
plt.bar([0, 1], weekend_label_distribution[1], bottom=weekend_label_distribution[0], label='Real News')

# Customize the chart
plt.xticks([0, 1], ['Weekday', 'Weekend'])
plt.xlabel('Day Type')
plt.ylabel('Number of Articles')
plt.title('Distribution of Real and Fake News by Weekend')
_ = plt.legend()

"""Weekends are more prone to fake news."""

# @title Month vs Label
monthly_label_distribution = train_df.groupby(['month', 'label']).size().unstack(fill_value=0)
monthly_label_distribution

# @title Real vs Fake across the months
monthly_label_distribution.plot(kind='bar', stacked=True, figsize=(10, 6))

# Customize the plot
plt.title('Monthly Distribution of Real and Fake News')
plt.xlabel('Month')
plt.ylabel('Number of Articles')
plt.xticks(rotation=0)  # Rotate x-axis labels for better readability
plt.legend(title='Label')  # Add a legend
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.show()

# @title Month vs label for test data
monthly_label_distribution = test_df.groupby(['month', 'label']).size().unstack(fill_value=0)
monthly_label_distribution

# @title Real vs Fake across the months for test data
monthly_label_distribution.plot(kind='bar', stacked=True, figsize=(10, 6))

# Customize the plot
plt.title('Monthly Distribution of Real and Fake News')
plt.xlabel('Month')
plt.ylabel('Number of Articles')
plt.xticks(rotation=0)  # Rotate x-axis labels for better readability
plt.legend(title='Label')  # Add a legend
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.show()

"""First 8 months (Jan - Aug) highly prone to fake news as a percentage of whole."""

# @title Suject vs Label
subject_label_distribution = train_df.groupby(['subject', 'label']).size().unstack(fill_value=0)
subject_label_distribution

# @title Real vs Fake across different subjects
subject_label_distribution.plot(kind='bar', stacked=True, figsize=(10, 6))

# Customize the plot
plt.title('Distribution of Real and Fake News vs Subject')
plt.xlabel('Subject')
plt.ylabel('Number of Articles')
plt.xticks(rotation=0)  # Rotate x-axis labels for better readability
plt.legend(title='Label')  # Add a legend
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.show()

!pip install textstat

# @title Feature extraction from title and text columns
from textblob import TextBlob
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.preprocessing import OneHotEncoder
import textstat

# 1. Title Features
def extract_title_features(df):
    df['title_char_count'] = df['title'].apply(len)  # Character count
    df['title_word_count'] = df['title'].apply(lambda x: len(x.split()))  # Word count
    df['title_sentiment'] = df['title'].apply(lambda x: TextBlob(x).sentiment.polarity)  # Sentiment score
    sensational_keywords = ['breaking', 'shocking', 'urgent', 'exclusive']
    df['title_keyword_density'] = df['title'].apply(
        lambda x: sum([x.lower().count(word) for word in sensational_keywords]) / len(x.split()) if len(x.split()) > 0 else 0
    )  # Keyword density
    return df

# 2. Text Features
def extract_text_features(df):
    df['text_word_count'] = df['text'].apply(lambda x: len(x.split()))  # Word count
    df['text_sentence_count'] = df['text'].apply(lambda x: len(re.split(r'[.!?]', x)))  # Sentence count
    df['text_sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)  # Sentiment score
    df['text_readability_score'] = df['text'].apply(textstat.flesch_reading_ease)  # Readability score
    return df

# Topic Modeling with LDA
def perform_topic_modeling(df, column, n_topics=5, top_words=10):
    """
    Performs topic modeling and extracts top words for each topic.

    Args:
        df: DataFrame with text data.
        column: Name of the text column.
        n_topics: Number of topics to extract.
        top_words: Number of top words to display for each topic.

    Returns:
        df: DataFrame with added topic columns.
        lda: Trained LDA model.
        topic_words: List of lists containing top words for each topic.
    """

    vectorizer = CountVectorizer(stop_words='english', max_features=5000)
    dt_matrix = vectorizer.fit_transform(df[column])
    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
    lda.fit(dt_matrix)
    topics = lda.transform(dt_matrix)

    # Get feature names (words)
    feature_names = vectorizer.get_feature_names_out()

    # Extract top words for each topic
    topic_words = []
    for topic_idx, topic in enumerate(lda.components_):
        top_words_idx = topic.argsort()[:-top_words - 1:-1]  # Get indices of top words
        top_words_for_topic = [feature_names[i] for i in top_words_idx]
        topic_words.append(top_words_for_topic)

    # Add topic columns to DataFrame
    for i in range(n_topics):
        df[f'topic_{i+1}'] = topics[:, i]

    return df, lda, topic_words

# Reload preprocessed datasets
train_df = pd.read_csv("preprocessed_train.tsv", sep='\t')
test_df = pd.read_csv("preprocessed_test.tsv", sep='\t')

# Title Features
train_df = extract_title_features(train_df)
test_df = extract_title_features(test_df)

# Text Features
train_df = extract_text_features(train_df)
test_df = extract_text_features(test_df)

# Topic Modeling (on 'text' column)
train_df, lda_model, topic_top_words = perform_topic_modeling(train_df, 'text', n_topics=5)
test_df, _, test_topic_top_words = perform_topic_modeling(test_df, 'text', n_topics=5)

# Save Feature-Engineered Data
train_df.to_csv("engineered_train.tsv", sep='\t', index=False)
test_df.to_csv("engineered_test.tsv", sep='\t', index=False)

print("Feature-Engineered Train Data saved as 'engineered_train.tsv'")
print("Feature-Engineered Test Data saved as 'engineered_test.tsv'")

# @title Plot of text sentiment vs title sentiment
plt.figure(figsize=(8, 6))
plt.scatter(train_df['text_sentiment'], train_df['title_sentiment'])
plt.xlabel('Text Sentiment')
plt.ylabel('Title Sentiment')
plt.title('Scatter Plot of Text Sentiment vs. Title Sentiment')
plt.grid(True)
plt.show()

# @title Comparison of text sentiment and title sentiment
train_df[['text_sentiment', 'title_sentiment']]

# @title Correlation between text sentimen and title sentiment

import seaborn as sns

plt.figure(figsize=(10, 8))
sns.heatmap(train_df[['text_sentiment', 'title_sentiment']].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap of Text Sentiment and Title Sentiment')
plt.show()

"""Title sentiment is not strongly correlated with text sentiment which does not give any conclusive evidence."""

# @title Plot of fake vs real news over time

import matplotlib.pyplot as plt

# Group data by date and label, then count occurrences
date_label_distribution_train = train_df.groupby(['date', 'label']).size().unstack(fill_value=0)
date_label_distribution_test = test_df.groupby(['date', 'label']).size().unstack(fill_value=0)


# Plotting for training data
plt.figure(figsize=(12, 6))
plt.plot(date_label_distribution_train.index, date_label_distribution_train[0], label='Fake')
plt.plot(date_label_distribution_train.index, date_label_distribution_train[1], label='Real')
plt.xlabel('Date')
plt.ylabel('Number of Articles')
plt.title('Trend of Real vs. Fake News Over Time (Training Data)')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Plotting for test data
plt.figure(figsize=(12, 6))
plt.plot(date_label_distribution_test.index, date_label_distribution_test[0], label='Fake')
plt.plot(date_label_distribution_test.index, date_label_distribution_test[1], label='Real')
plt.xlabel('Date')
plt.ylabel('Number of Articles')
plt.title('Trend of Real vs. Fake News Over Time (Test Data)')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Amount of fake news remais same over the year, although the ratio of fake news to real news decreases."""

# @title Readability score and Avergae Sentence length comparison

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming train_df and test_df are already loaded and preprocessed

# Calculate readability scores
train_df['readability_score'] = train_df['text'].apply(textstat.flesch_reading_ease)
test_df['readability_score'] = test_df['text'].apply(textstat.flesch_reading_ease)


# Boxplot comparison
plt.figure(figsize=(10, 6))
sns.boxplot(x='label', y='readability_score', data=train_df)
plt.title('Readability Scores by Label (Training Data)')
plt.xlabel('Label (0: Fake, 1: Real)')
plt.ylabel('Readability Score')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='label', y='readability_score', data=test_df)
plt.title('Readability Scores by Label (Test Data)')
plt.xlabel('Label (0: Fake, 1: Real)')
plt.ylabel('Readability Score')
plt.show()


#Further analysis (example: sentence length)
train_df['avg_sentence_length'] = train_df['text'].apply(lambda x: sum(len(s.split()) for s in re.split(r'[.!?]', x)) / len(re.split(r'[.!?]', x)) if len(re.split(r'[.!?]', x)) > 0 else 0)
test_df['avg_sentence_length'] = test_df['text'].apply(lambda x: sum(len(s.split()) for s in re.split(r'[.!?]', x)) / len(re.split(r'[.!?]', x)) if len(re.split(r'[.!?]', x)) > 0 else 0)

plt.figure(figsize=(10, 6))
sns.boxplot(x='label', y='avg_sentence_length', data=train_df)
plt.title('Average Sentence Length by Label (Training Data)')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='label', y='avg_sentence_length', data=test_df)
plt.title('Average Sentence Length by Label (Test Data)')
plt.show()

"""1. Not much difference in readability scores and average lengths of the text.
2. Although it can be seen that fake news labe has many outliers showing inconsistency in the writting skills.
3. Real news are a bit complex and have a bit longer text lengths.
"""

# @title Linguistic features extraction

import nltk
nltk.download('averaged_perceptron_tagger_eng') # Download the necessary data package

def extract_linguistic_features(df):
    """
    Extracts linguistic features from text data.

    Args:
        df: DataFrame with 'title' and 'text' columns.

    Returns:
        DataFrame with added linguistic features.
    """

    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    def process_text(text):
        words = word_tokenize(text.lower())
        words = [lemmatizer.lemmatize(w) for w in words if w.isalnum() and w not in stop_words]
        return words

    df['title_words'] = df['title'].apply(process_text)
    df['text_words'] = df['text'].apply(process_text)

    # Word Frequency
    df['title_word_freq'] = df['title_words'].apply(lambda x: nltk.FreqDist(x))
    df['text_word_freq'] = df['text_words'].apply(lambda x: nltk.FreqDist(x))

    # Most Frequent Words
    df['title_most_common'] = df['title_word_freq'].apply(lambda x: x.most_common(5)) # Top 5
    df['text_most_common'] = df['text_word_freq'].apply(lambda x: x.most_common(5)) # Top 5

    # Part-of-Speech Tagging
    df['title_pos_tags'] = df['title_words'].apply(lambda x: nltk.pos_tag(x))
    df['text_pos_tags'] = df['text_words'].apply(lambda x: nltk.pos_tag(x))


    return df

# Example usage (assuming train_df and test_df are already loaded):
train_df = extract_linguistic_features(train_df)
test_df = extract_linguistic_features(test_df)

#Further analysis (example: POS tag distribution)
def pos_tag_analysis(df):
  pos_counts = {}

  for pos_list in df['text_pos_tags']:
      for word, pos in pos_list:
          if pos in pos_counts:
              pos_counts[pos] += 1
          else:
              pos_counts[pos] = 1

  sorted_pos_counts = dict(sorted(pos_counts.items(), key=lambda item: item[1], reverse=True))
  print("Text POS tag frequency distribution")
  print(sorted_pos_counts)

pos_tag_analysis(train_df)

# @title POS tags in table form

import pandas as pd

def pos_tag_analysis(df):
  pos_counts = {}

  for pos_list in df['text_pos_tags']:
      for word, pos in pos_list:
          if pos in pos_counts:
              pos_counts[pos] += 1
          else:
              pos_counts[pos] = 1

  sorted_pos_counts = dict(sorted(pos_counts.items(), key=lambda item: item[1], reverse=True))

  # Create a DataFrame for better visualization
  pos_df = pd.DataFrame(list(sorted_pos_counts.items()), columns=['POS Tag', 'Frequency'])
  print(pos_df)

pos_tag_analysis(train_df)

# @title Separate pos analysis for fake and real news

import nltk
import pandas as pd

def pos_tag_analysis_by_label(df):
    """
    Performs POS tag analysis separately for fake and real news.
    """
    nltk.download('averaged_perceptron_tagger_eng', quiet=True)

    # Group data by label
    fake_news = df[df['label'] == 0]
    real_news = df[df['label'] == 1]

    def analyze_pos_tags(news_df):
        pos_counts = {}
        for pos_list in news_df['text_pos_tags']:
            for word, pos in pos_list:
                if pos in pos_counts:
                    pos_counts[pos] += 1
                else:
                    pos_counts[pos] = 1

        return pos_counts

    fake_pos = analyze_pos_tags(fake_news)
    real_pos = analyze_pos_tags(real_news)

    # Convert to dataframes for better visualization
    fake_pos_df = pd.DataFrame(list(fake_pos.items()), columns=['POS Tag', 'Frequency'])
    real_pos_df = pd.DataFrame(list(real_pos.items()), columns=['POS Tag', 'Frequency'])

    print("POS Tag Analysis for Fake News:")
    print(fake_pos_df)

    print("\nPOS Tag Analysis for Real News:")
    print(real_pos_df)

pos_tag_analysis_by_label(train_df)

# @title POS tag analysis for fake and real news

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd # make sure to import pandas
import nltk # make sure to import nltk

def pos_tag_analysis_by_label(df):
    """
    Performs POS tag analysis separately for fake and real news.
    """
    nltk.download('averaged_perceptron_tagger_eng', quiet=True)

    # Group data by label
    fake_news = df[df['label'] == 0]
    real_news = df[df['label'] == 1]

    def analyze_pos_tags(news_df):
        pos_counts = {}
        for pos_list in news_df['text_pos_tags']:
            for word, pos in pos_list:
                if pos in pos_counts:
                    pos_counts[pos] += 1
                else:
                    pos_counts[pos] = 1

        return pos_counts

    # Call analyze_pos_tags here to define fake_pos and real_pos
    fake_pos = analyze_pos_tags(fake_news)
    real_pos = analyze_pos_tags(real_news)

    # Convert to DataFrames
    fake_pos_df = pd.DataFrame(list(fake_pos.items()), columns=['POS Tag', 'Fake Frequency'])
    real_pos_df = pd.DataFrame(list(real_pos.items()), columns=['POS Tag', 'Real Frequency'])

    # Merge the DataFrames
    merged_df = pd.merge(fake_pos_df, real_pos_df, on='POS Tag', how='outer').fillna(0)

    # Calculate the difference in frequencies
    merged_df['Difference'] = merged_df['Fake Frequency'] - merged_df['Real Frequency']

    # Create a bar plot for comparison
    plt.figure(figsize=(12, 6))
    sns.barplot(x='POS Tag', y='Difference', data=merged_df)
    plt.title('Difference in POS Tag Frequencies (Fake - Real)')
    plt.xlabel('POS Tag')
    plt.ylabel('Frequency Difference')
    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
    plt.tight_layout()
    plt.show()

    # Create a heatmap for comparison
    pos_tags = merged_df['POS Tag']
    frequencies = merged_df[['Fake Frequency', 'Real Frequency']]
    plt.figure(figsize=(10, 6))
    sns.heatmap(frequencies, annot=True, fmt=".0f", cmap='coolwarm', xticklabels=['Fake', 'Real'], yticklabels=pos_tags)
    plt.title('POS Tag Frequencies Comparison (Fake vs. Real)')
    plt.xlabel('News Type')
    plt.ylabel('POS Tag')
    plt.show()


# Call the function with your dataframe
pos_tag_analysis_by_label(train_df)

"""Exploratory Insights
Stylistic Differences:
Fake news appears more emotive and dramatic (higher use of adverbs, and less proper nouns).
Real news appears more factual and event-focused (higher verb usage and proper nouns).
This aligns with the hypothesis that fake news uses sensationalized language to capture attention, whereas real news emphasizes specifics and facts.
"""

# @title Part of Speech frequency addition to main dataframe

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import nltk

# Ensure nltk resources are downloaded (if not already)
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Feature Engineering (POS Tag Frequencies)
def pos_freq_features(df):
    """
    Extracts and adds POS tag frequency features to the DataFrame.
    """
    from nltk.tokenize import word_tokenize  # Import word_tokenize here
    from nltk.corpus import stopwords  # Import stopwords here
    from nltk.stem import WordNetLemmatizer  # Import WordNetLemmatizer here

    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    def process_text(text):
        words = word_tokenize(text.lower())  # Use word_tokenize
        words = [lemmatizer.lemmatize(w) for w in words if w.isalnum() and w not in stop_words]
        return words

    # Re-create the 'text_pos_tags' column if needed
    if 'text_pos_tags' not in df.columns:
        df['title_words'] = df['title'].apply(process_text)
        df['text_words'] = df['text'].apply(process_text)
        df['text_pos_tags'] = df['text_words'].apply(lambda x: nltk.pos_tag(x))  # Recalculate text_pos_tags

    pos_features = {}
    for index, row in df.iterrows():
        pos_counts = {}
        for word, pos in row['text_pos_tags']:
            pos_counts[pos] = pos_counts.get(pos, 0) + 1

        for pos, count in pos_counts.items():
            col_name = f"pos_{pos}"
            if col_name not in df.columns:
                df[col_name] = 0
            df.loc[index, col_name] = count
    return df

train_df = pos_freq_features(train_df)
test_df = pos_freq_features(test_df)

# Ensure both train and test have the same columns:
train_cols = set(train_df.columns)
test_cols = set(test_df.columns)

# Add missing columns to test_df, filled with 0s
for col in train_cols - test_cols:
    test_df[col] = 0

# Add missing columns to train_df, filled with 0s
for col in test_cols - train_cols:
    train_df[col] = 0

train_df = pos_freq_features(train_df)
test_df = pos_freq_features(test_df)

# Get all unique columns from both DataFrames
all_cols = list(set(train_df.columns) | set(test_df.columns))

# Reindex both DataFrames to ensure they have the same columns in the same order
# Fill missing values with 0
train_df = train_df.reindex(columns=all_cols, fill_value=0)
test_df = test_df.reindex(columns=all_cols, fill_value=0)

# @title Regression models for POS analysis

# Prepare data for modeling
X_train = train_df.drop(['label','title','text','date', 'title_words','text_words','title_word_freq','text_word_freq','title_most_common','text_most_common','title_pos_tags', 'text_pos_tags', 'subject'], axis=1) # Dropping 'subject', 'text_pos_tags', 'title_pos_tags', and other non-numerical columns
y_train = train_df['label']
X_test = test_df.drop(['label','title','text','date', 'title_words','text_words','title_word_freq','text_word_freq','title_most_common','text_most_common', 'title_pos_tags', 'text_pos_tags','subject'], axis=1) # Dropping 'subject', 'text_pos_tags', 'title_pos_tags', and other non-numerical columns
y_test = test_df['label']

# Handle missing values (if any)
X_train.fillna(0, inplace=True)
X_test.fillna(0, inplace=True)

# Logistic Regression
logreg_model = LogisticRegression(max_iter=1000)  # Increased max_iter
logreg_model.fit(X_train, y_train)
logreg_predictions = logreg_model.predict(X_test)

print("Logistic Regression Accuracy:", accuracy_score(y_test, logreg_predictions))
print(classification_report(y_test, logreg_predictions))

# Random Forest
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)

print("\nRandom Forest Accuracy:", accuracy_score(y_test, rf_predictions))
print(classification_report(y_test, rf_predictions))

# @title Feature importance of Random Forest

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'rf_model' is your trained RandomForestClassifier
feature_importances = rf_model.feature_importances_

# Create a DataFrame for better visualization
feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plotting the top N features
top_n = 20  # Change this to display more or fewer features
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(top_n))
plt.title(f'Top {top_n} Feature Importances from Random Forest')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# Print the feature importances
print(feature_importance_df.head(top_n))

"""News type (subject), topic_1 from LDA, word count of both the title and texts are important features in the Random Forest model trained on Parts of Speech"""

# @title Topic wise top words
topic_words_df = pd.DataFrame(topic_top_words).T  # Transpose for better table format
topic_words_df.columns = [f'Topic {i+1}' for i in range(len(topic_words_df.columns))]
topic_words_df

# @title POS tag frequency vs. sentiment
plt.figure(figsize=(10, 6))
sns.boxplot(x='label', y='pos_NN', data=train_df)  # Example: Noun frequency
plt.title('Noun Frequency by Label')
plt.show()

"""It can be seen that Real News uses more nouns than Fake news, with concentrated frequence where as in Fke news there are many outliers showing inconsistencies in writting style."""

# @title Relationship between specific POS tags and readability
plt.figure(figsize=(10, 6))
sns.scatterplot(x='pos_JJ', y='readability_score', hue='label', data=train_df) # Example: Adjectives vs. Readability
plt.title('Adjective Frequency vs. Readability Score')
plt.show()

"""Clearly, adjectives and readablitiy scores are negatively correlated for both real and fake news."""

# @title SVM
from sklearn.svm import SVC

# Support Vector Machine (SVM)
svm_model = SVC(kernel='linear')  # Try different kernels
svm_model.fit(X_train, y_train)
svm_predictions = svm_model.predict(X_test)

print("\nSVM Accuracy:", accuracy_score(y_test, svm_predictions))
print(classification_report(y_test, svm_predictions))

# @title Best parameters of above Random Forest model
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier # Import RandomForestClassifier

# Example: Tuning Random Forest
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)
grid_search.fit(X_train, y_train)

print("\nBest Random Forest Parameters:", grid_search.best_params_)
best_rf_model = grid_search.best_estimator_
best_rf_predictions = best_rf_model.predict(X_test)
print("\nBest Random Forest Accuracy:", accuracy_score(y_test, best_rf_predictions))


# Feature Importance Visualization (improved)
importances = best_rf_model.feature_importances_
indices = np.argsort(importances)[::-1]
names = [X_train.columns[i] for i in indices]

# Create plot
plt.figure(figsize=(12, 6))
plt.title("Feature Importance")
plt.bar(range(X_train.shape[1]), importances[indices])
plt.xticks(range(X_train.shape[1]), names, rotation=90)
plt.show()


# Explore interactions between features

# Example: Pairplot for key features
sns.pairplot(train_df[['pos_NN', 'pos_VB', 'readability_score', 'label']], hue='label')
plt.show()

# @title Save the engineered datasets to CSV files.
train_df.to_csv('engineered_train.csv', index=False)
test_df.to_csv('engineered_test.csv', index=False)

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score, f1_score
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report

# @title Function to prepare TF-IDF features
def prepare_tfidf_features(df, column, max_features=5000):
    vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english')
    tfidf_matrix = vectorizer.fit_transform(df[column])
    return tfidf_matrix, vectorizer

# Target variable and features
y_train = train_df['label'].replace('Missing', np.nan).astype(float)
y_train = y_train[np.isfinite(y_train)].astype(int)  # Filter out non-finite values before casting to int

X_train_title_tfidf, title_vectorizer = prepare_tfidf_features(train_df, 'title')
X_train_text_tfidf, text_vectorizer = prepare_tfidf_features(train_df, 'text')

# Combine TF-IDF features, ensuring the same number of rows
X_train_combined = np.hstack([X_train_title_tfidf.toarray(), X_train_text_tfidf.toarray()])
X_train_combined = X_train_combined[np.isfinite(train_df['label'].replace('Missing', np.nan).astype(float))]  # Filter X_train_combined

# @title Logistic Regression
def train_logistic_regression(X_train, y_train):
    model = LogisticRegression(max_iter=1000, random_state=42)
    model.fit(X_train, y_train)
    return model

# Logistic Regression
lr_model = train_logistic_regression(X_train_combined, y_train)

# @title Random Forest
def train_random_forest(X_train, y_train):
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    return model

# Main Function for Model Building

# Random Forest
rf_model = train_random_forest(X_train_combined, y_train)

# @title LSTM Model
def train_lstm(X_train, y_train, max_features=5000, max_len=200, embedding_dim=128, batch_size=32, epochs=5):

    X_train = X_train[y_train.index]  # Align X_train with y_train using index
    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features)
    tokenizer.fit_on_texts(X_train)
    X_train_seq = tokenizer.texts_to_sequences(X_train)
    X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(X_train_seq, maxlen=max_len)

    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=max_len),
        tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # Convert y_train to numerical values (0 or 1)
    # Replace 'Missing' with a numerical value, e.g., -1 or np.nan
    # and then convert to integers (0 or 1) with appropriate handling of missing values
    y_train = y_train.replace('Missing', -1).astype(float).astype(int)

    model.fit(X_train_padded, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)
    return model, tokenizer

# Optional: Train LSTM
print("\nTraining LSTM")
lstm_model, lstm_tokenizer = train_lstm(train_df['text'], y_train)

# @title Model evaluation function

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
import numpy as np

def evaluate_model(y_true, y_pred, y_prob=None, model_name="Model"):
    """
    Evaluate and print metrics for a classification model.

    Parameters:
    y_true (array): True labels from the test set.
    y_pred (array): Predicted labels by the model.
    y_prob (array): Predicted probabilities for the positive class (optional for AUC-ROC).
    model_name (str): Name of the model for reporting.
    """
    print(f"\n{model_name} Evaluation:")
    print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
    print(f"Precision: {precision_score(y_true, y_pred):.4f}")
    print(f"Recall: {recall_score(y_true, y_pred):.4f}")
    print(f"F1 Score: {f1_score(y_true, y_pred):.4f}")
    if y_prob is not None:
        print(f"AUC-ROC: {roc_auc_score(y_true, y_prob):.4f}")
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred))

# @title TF-IDF
X_test_title_tfidf = title_vectorizer.transform(test_df['title'])
X_test_text_tfidf = text_vectorizer.transform(test_df['text'])
X_test_combined = np.hstack([X_test_title_tfidf.toarray(), X_test_text_tfidf.toarray()])

# @title Logistic Regression results
lr_predictions = lr_model.predict(X_test_combined)
lr_probabilities = lr_model.predict_proba(X_test_combined)[:, 1]

# Evaluate
evaluate_model(test_df['label'], lr_predictions, lr_probabilities, model_name="Logistic Regression")

# @title Random Forest Model results
rf_predictions = rf_model.predict(X_test_combined)
rf_probabilities = rf_model.predict_proba(X_test_combined)[:, 1]

# Evaluate
evaluate_model(test_df['label'], rf_predictions, rf_probabilities, model_name="Random Forest")

# @title LSTM model results
max_len = 200
X_test_seq = lstm_tokenizer.texts_to_sequences(test_df['text'])  # Use tokenizer trained on train_df
X_test_padded = tf.keras.preprocessing.sequence.pad_sequences(X_test_seq, maxlen=max_len)

# Get predictions
lstm_probabilities = lstm_model.predict(X_test_padded).flatten()  # Output probabilities
lstm_predictions = (lstm_probabilities > 0.5).astype(int)  # Convert probabilities to binary predictions

# Evaluate
evaluate_model(test_df['label'], lstm_predictions, lstm_probabilities, model_name="LSTM")

# @title Confusion matrices for the three models

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

def plot_confusion_matrix(model, X_test, y_test, model_name):
    """
    Plots the confusion matrix for a given model.

    Parameters:
    model: The trained classification model.
    X_test: The test data.
    y_test: The true labels for the test data.
    model_name: The name of the model (for the plot title).
    """
    # Get predictions - handle LSTM differently
    if model_name == "LSTM":
        y_pred = (model.predict(X_test) > 0.5).astype(int)  # Convert LSTM probabilities to binary predictions
    else:
        y_pred = model.predict(X_test)

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Real", "Fake"])
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f"Confusion Matrix: {model_name}")
    plt.show()

models = {
    "Logistic Regression": lr_model,
    "Random Forest": rf_model,
    "LSTM": lstm_model
}

y_test = test_df['label'].replace('Missing', np.nan).astype(float)
y_test = y_test[np.isfinite(y_test)].astype(int)  # Filter out non-finite values before casting to int

X_test_padded_sliced = X_test_padded[:len(y_test)]

for model_name, model in models.items():
    print(f"Confusion Matrix for {model_name}:")
    if model_name == "LSTM":
        # Use the sliced padded sequences for LSTM
        plot_confusion_matrix(model, X_test_padded_sliced, y_test, model_name)
    else:
        # Use the combined TF-IDF features for other models
        plot_confusion_matrix(model, X_test_combined, y_test, model_name)

# @title ROC curve
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

def plot_roc_curve(models, X_test_data, y_test):
    plt.figure(figsize=(10, 6))
    for model_name, model in models.items():
        # Get the appropriate X_test data for the current model
        X_test = X_test_data.get(model_name)

        if model_name == "LSTM":
            y_pred_prob = model.predict(X_test).flatten()
        else:
            y_pred_prob = model.predict_proba(X_test)[:, 1]

        fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"{model_name} (AUC = {roc_auc:.2f})")

    plt.plot([0, 1], [0, 1], 'k--')
    plt.title("ROC Curve")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend(loc="lower right")
    plt.show()

plot_roc_curve(models, {"Logistic Regression": X_test_combined, "Random Forest": X_test_combined, "LSTM": X_test_padded}, test_df['label'])

# @title Precision-Recall curve
from sklearn.metrics import precision_recall_curve

def plot_precision_recall_curve(models, X_test, y_test):
    plt.figure(figsize=(10, 6))
    for model_name, model in models.items():
        y_pred_prob = model.predict_proba(X_test)[:, 1]
        precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)
        plt.plot(recall, precision, label=f"{model_name}")
    plt.title("Precision-Recall Curve")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.legend(loc="best")
    plt.show()

models_without_lstm = {k: v for k, v in models.items() if k != "LSTM"}
plot_precision_recall_curve(models_without_lstm, X_test_combined, y_test)

# @title R-squared values of the three models

from sklearn.metrics import r2_score
import numpy as np # Added import for numpy

# Assuming you have your true labels (y_test) and predictions (y_pred) for each model
# Define y_test here to make it accessible
y_test = test_df['label'].replace('Missing', np.nan).astype(float)
y_test = y_test[np.isfinite(y_test)].astype(int)  # Filter out non-finite values before casting to int


# Calculate R-squared for Logistic Regression
lr_r2 = r2_score(y_test, lr_predictions)
print(f"Logistic Regression R-squared: {lr_r2}")

# Calculate R-squared for Random Forest
rf_r2 = r2_score(y_test, rf_predictions)
print(f"Random Forest R-squared: {rf_r2}")

# Calculate R-squared for LSTM (if applicable, adjust based on your predictions)
lstm_r2 = r2_score(y_test, lstm_predictions)
print(f"LSTM R-squared: {lstm_r2}")

# @title Result file of Logistic regression as it gave the best results

results = []
test_data_df = pd.read_csv("test.tsv", sep="\t")
# Assuming lr_predictions contains the predictions from the Logistic Regression model
# and test_df['title'] contains the titles from your test data
for title, prediction in zip(test_data_df['title'], lr_predictions):
    results.append([title, prediction])

# Example output formatting (replace with your actual results)
with open('result.txt', 'w') as f:
    f.write("{\n")
    for result in results:
        f.write(f'    ["{result[0]}", {result[1]}],\n')
    f.write("}\n")

# Alternatively, using json for cleaner output:
import json
with open('result.json', 'w') as f:
  # Convert NumPy int64 to Python int before dumping
  json.dump([[r[0], int(r[1])] for r in results], f, indent=4)

# @title Evaluation of Logistic regression - Result.txt based on the test file (preprocessed using the above code)

import json
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve, auc, precision_recall_curve, precision_score, recall_score, f1_score, r2_score
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Load the results from the result.txt file
# Assuming the 'result.txt' file is in the correct format as written in previous code
try:
  with open('result.json', 'r') as f:
    results = json.load(f)
except FileNotFoundError:
  print("Error: 'result.json' not found. Please ensure the file exists and contains the predictions.")
  # Handle the error appropriately, e.g. exit the program or provide a default value
  exit(1) #or some other method to gracefully exit

# Extract predictions from the result
y_pred = [result[1] for result in results]

# Load the actual labels (y_test) from your test dataset
# Replace 'test.tsv' with the actual filename
try:
    y_test = test_df['label'].values
except FileNotFoundError:
    print("Error: 'test.tsv' not found.  Please ensure the file exists.")
    exit(1)


# Ensure y_pred and y_test have the same length
if len(y_pred) != len(y_test):
    print("Error: Number of predictions and actual labels do not match.")
    print(f"Number of Predictions: {len(y_pred)}")
    print(f"Number of Actual labels: {len(y_test)}")
    # Handle this situation. Options include truncating, padding, or raising an error.
    min_len = min(len(y_pred),len(y_test))
    y_pred = y_pred[:min_len]
    y_test = y_test[:min_len]

#Evaluate model
# Evaluate the model
print("Evaluation using result.txt:")
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

#Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Real", "Fake"])
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix from result.txt")
plt.show()

import json
results = []

# Function to generate results for different models
def create_result_file(model_name, predictions):
  result_file = f'{model_name}_result.json'
  model_results = []
  for title, prediction in zip(test_df['title'], predictions):
      model_results.append([title, int(prediction)])  # Ensure prediction is an integer

  with open(result_file, 'w') as f:
      json.dump(model_results, f, indent=4)
  return result_file


# Create result files for each model
rf_result_file = create_result_file("RandomForest", rf_predictions)
lstm_result_file = create_result_file("LSTM", lstm_predictions)

print(f"Random Forest results saved to: {rf_result_file}")
print(f"LSTM results saved to: {lstm_result_file}")

# @title Evaluation of other models based on their respective result files

def evaluate_model_from_result_file(result_file, y_test):
  try:
      with open(result_file, 'r') as f:
          results = json.load(f)
  except FileNotFoundError:
      print(f"Error: '{result_file}' not found.")
      return

  y_pred = [result[1] for result in results]

  if len(y_pred) != len(y_test):
      print(f"Error in {result_file}: Number of predictions and actual labels do not match.")
      return

  print(f"\nEvaluation of {result_file}:")
  print("Accuracy:", accuracy_score(y_test, y_pred))
  print(classification_report(y_test, y_pred))

  cm = confusion_matrix(y_test, y_pred)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Real", "Fake"])
  disp.plot(cmap=plt.cm.Blues)
  plt.title(f"Confusion Matrix from {result_file}")
  plt.show()

# Load actual labels (ensure y_test is defined correctly)
y_test = test_df['label'].replace('Missing', np.nan).astype(float)
y_test = y_test[np.isfinite(y_test)].astype(int)

# Evaluate each model's results
evaluate_model_from_result_file(rf_result_file, y_test)
evaluate_model_from_result_file(lstm_result_file, y_test)

